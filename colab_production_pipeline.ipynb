{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pallet + Box Detection/Segmentation/Tracking Pipeline (Colab-Ready)\n",
    "\n",
    "This notebook is a **single production architecture** optimized for Google Colab (Free/Pro) using:\n",
    "- **Grounding DINO** (`IDEA-Research/grounding-dino-base`) for text-conditioned detection.\n",
    "- **SAM ViT-B** for segmentation.\n",
    "- **Custom centroid tracker** for persistent IDs across frames.\n",
    "\n",
    "It produces:\n",
    "1. Annotated output video.\n",
    "2. `counts.json` with per-frame tracked IDs + counts.\n",
    "3. Interactive analytics section with Matplotlib + Plotly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 1 — Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 1 — Environment Setup\n",
    "# =========================\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def run_cmd(cmd: str):\n",
    "    print(f\"[CMD] {cmd}\")\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "run_cmd(\"pip -q install --upgrade pip\")\n",
    "run_cmd(\"pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
    "run_cmd(\"pip -q install transformers accelerate opencv-python tqdm matplotlib plotly imageio pandas\")\n",
    "run_cmd(\"pip -q install git+https://github.com/facebookresearch/segment-anything.git\")\n",
    "\n",
    "if not Path('/content/segment-anything').exists():\n",
    "    run_cmd('git clone -q https://github.com/facebookresearch/segment-anything.git /content/segment-anything')\n",
    "\n",
    "sam_ckpt = Path('/content/sam_vit_b_01ec64.pth')\n",
    "if not sam_ckpt.exists():\n",
    "    run_cmd('wget -q -O /content/sam_vit_b_01ec64.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "print('[INFO] Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 2 — Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 2 — Global Configuration\n",
    "# =========================\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('/content/drive/MyDrive/pallet_cv_project')\n",
    "INPUT_VIDEO = PROJECT_ROOT / 'input' / 'drone_video.mp4'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'results'\n",
    "SAM_CHECKPOINT = Path('/content/sam_vit_b_01ec64.pth')\n",
    "\n",
    "RESIZE_FACTOR = 1.0\n",
    "PROCESS_EVERY_N_FRAMES = 1\n",
    "LOG_EVERY_N_FRAMES = 25\n",
    "DRAW_MASK_OVERLAY = False\n",
    "BOX_THRESHOLD = 0.30\n",
    "TEXT_THRESHOLD = 0.25\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "GPU_NAME = torch.cuda.get_device_name(0) if DEVICE == 'cuda' else 'CPU'\n",
    "GPU_MEM_GB = (torch.cuda.get_device_properties(0).total_memory / (1024**3)) if DEVICE == 'cuda' else 0\n",
    "\n",
    "ALLOW_FULL_RES = GPU_MEM_GB > 16\n",
    "USE_AMP = GPU_MEM_GB > 16 and DEVICE == 'cuda'\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f'[INFO] DEVICE={DEVICE} | GPU={GPU_NAME} | VRAM={GPU_MEM_GB:.1f} GB')\n",
    "print(f'[INFO] Input video: {INPUT_VIDEO}')\n",
    "print(f'[INFO] Output dir : {OUTPUT_DIR}')\n",
    "print(f'[INFO] ALLOW_FULL_RES={ALLOW_FULL_RES} | USE_AMP={USE_AMP}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 3 — Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 3 — Model Loading\n",
    "# =========================\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "DINO_MODEL_ID = 'IDEA-Research/grounding-dino-base'\n",
    "\n",
    "try:\n",
    "    print('[INFO] Loading Grounding DINO...')\n",
    "    dino_processor = AutoProcessor.from_pretrained(DINO_MODEL_ID)\n",
    "    dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(DINO_MODEL_ID).to(DEVICE).eval()\n",
    "\n",
    "    print('[INFO] Loading SAM ViT-B...')\n",
    "    sam = sam_model_registry['vit_b'](checkpoint=str(SAM_CHECKPOINT))\n",
    "    sam.to(device=DEVICE)\n",
    "    sam.eval()\n",
    "    sam_predictor = SamPredictor(sam)\n",
    "    print('[INFO] Models loaded successfully.')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Model loading failed: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 4 — Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 4 — Utility Functions\n",
    "# =========================\n",
    "import cv2\n",
    "import numpy as np\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "def cleanup_cuda():\n",
    "    gc.collect()\n",
    "    if DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def detect_objects(image_bgr: np.ndarray, prompt: str,\n",
    "                   box_threshold: float = BOX_THRESHOLD,\n",
    "                   text_threshold: float = TEXT_THRESHOLD) -> Tuple[np.ndarray, List[str], np.ndarray]:\n",
    "    \"\"\"Grounding DINO inference with compatibility across transformers versions.\"\"\"\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    with torch.no_grad():\n",
    "        inputs = dino_processor(images=image_rgb, text=prompt, return_tensors='pt').to(DEVICE)\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            outputs = dino_model(**inputs)\n",
    "\n",
    "        target_sizes = torch.tensor([image_rgb.shape[:2]], device=DEVICE)\n",
    "\n",
    "        # Compatibility: different transformers versions expose different argument names.\n",
    "        pp_fn = dino_processor.post_process_grounded_object_detection\n",
    "        pp_sig = inspect.signature(pp_fn)\n",
    "        kwargs = {\n",
    "            'outputs': outputs,\n",
    "            'input_ids': inputs.input_ids,\n",
    "            'target_sizes': target_sizes\n",
    "        }\n",
    "        if 'box_threshold' in pp_sig.parameters:\n",
    "            kwargs['box_threshold'] = box_threshold\n",
    "        elif 'threshold' in pp_sig.parameters:\n",
    "            kwargs['threshold'] = box_threshold\n",
    "\n",
    "        if 'text_threshold' in pp_sig.parameters:\n",
    "            kwargs['text_threshold'] = text_threshold\n",
    "\n",
    "        results = pp_fn(**kwargs)[0]\n",
    "\n",
    "    boxes = results['boxes'].detach().cpu().numpy() if len(results['boxes']) else np.empty((0, 4), dtype=np.float32)\n",
    "    labels = list(results['labels']) if 'labels' in results else []\n",
    "    scores = results['scores'].detach().cpu().numpy() if len(results['scores']) else np.empty((0,), dtype=np.float32)\n",
    "    del inputs, outputs, results, target_sizes\n",
    "    cleanup_cuda()\n",
    "    return boxes, labels, scores\n",
    "\n",
    "\n",
    "def segment_boxes(image_bgr: np.ndarray, boxes_xyxy: np.ndarray) -> List[np.ndarray]:\n",
    "    if len(boxes_xyxy) == 0:\n",
    "        return []\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "    masks_out = []\n",
    "    for box in boxes_xyxy:\n",
    "        masks, scores, _ = sam_predictor.predict(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            box=np.array(box, dtype=np.float32)[None, :],\n",
    "            multimask_output=False\n",
    "        )\n",
    "        masks_out.append(masks[0].astype(np.uint8))\n",
    "    del image_rgb\n",
    "    cleanup_cuda()\n",
    "    return masks_out\n",
    "\n",
    "\n",
    "def filter_masks(masks: List[np.ndarray], min_area: int = 120, max_area_ratio: float = 0.8) -> List[np.ndarray]:\n",
    "    if not masks:\n",
    "        return []\n",
    "    h, w = masks[0].shape[:2]\n",
    "    max_area = int(h * w * max_area_ratio)\n",
    "    return [m for m in masks if min_area <= int(m.sum()) <= max_area]\n",
    "\n",
    "\n",
    "def get_centroid(mask: np.ndarray):\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0:\n",
    "        return None\n",
    "    return int(xs.mean()), int(ys.mean())\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrackState:\n",
    "    centroid: Tuple[int, int]\n",
    "    disappeared: int = 0\n",
    "\n",
    "\n",
    "class CentroidTracker:\n",
    "    def __init__(self, max_disappeared: int = 15, max_distance: float = 80.0):\n",
    "        self.next_id = 0\n",
    "        self.objects: Dict[int, TrackState] = {}\n",
    "        self.max_disappeared = max_disappeared\n",
    "        self.max_distance = max_distance\n",
    "\n",
    "    def _register(self, centroid):\n",
    "        self.objects[self.next_id] = TrackState(centroid=centroid, disappeared=0)\n",
    "        self.next_id += 1\n",
    "\n",
    "    def _deregister(self, object_id):\n",
    "        self.objects.pop(object_id, None)\n",
    "\n",
    "    def update(self, input_centroids: List[Tuple[int, int]]):\n",
    "        if len(input_centroids) == 0:\n",
    "            for object_id in list(self.objects.keys()):\n",
    "                self.objects[object_id].disappeared += 1\n",
    "                if self.objects[object_id].disappeared > self.max_disappeared:\n",
    "                    self._deregister(object_id)\n",
    "            return self.objects\n",
    "\n",
    "        if len(self.objects) == 0:\n",
    "            for c in input_centroids:\n",
    "                self._register(c)\n",
    "            return self.objects\n",
    "\n",
    "        object_ids = list(self.objects.keys())\n",
    "        object_centroids = np.array([self.objects[i].centroid for i in object_ids])\n",
    "        new_centroids = np.array(input_centroids)\n",
    "        D = np.linalg.norm(object_centroids[:, None] - new_centroids[None, :], axis=2)\n",
    "\n",
    "        rows = D.min(axis=1).argsort()\n",
    "        cols = D.argmin(axis=1)[rows]\n",
    "        used_rows, used_cols = set(), set()\n",
    "\n",
    "        for row, col in zip(rows, cols):\n",
    "            if row in used_rows or col in used_cols or D[row, col] > self.max_distance:\n",
    "                continue\n",
    "            object_id = object_ids[row]\n",
    "            self.objects[object_id].centroid = tuple(new_centroids[col])\n",
    "            self.objects[object_id].disappeared = 0\n",
    "            used_rows.add(row)\n",
    "            used_cols.add(col)\n",
    "\n",
    "        for row in set(range(D.shape[0])) - used_rows:\n",
    "            object_id = object_ids[row]\n",
    "            self.objects[object_id].disappeared += 1\n",
    "            if self.objects[object_id].disappeared > self.max_disappeared:\n",
    "                self._deregister(object_id)\n",
    "\n",
    "        for col in set(range(D.shape[1])) - used_cols:\n",
    "            self._register(tuple(new_centroids[col]))\n",
    "\n",
    "        return self.objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 5 — Optimized Video Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 5 — Optimized Video Processing Pipeline\n",
    "# =========================\n",
    "from tqdm import tqdm\n",
    "\n",
    "video_in = str(INPUT_VIDEO)\n",
    "video_out = str(OUTPUT_DIR / 'annotated_output.mp4')\n",
    "json_out = str(OUTPUT_DIR / 'counts.json')\n",
    "\n",
    "cap = cv2.VideoCapture(video_in)\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f'Cannot open video: {video_in}')\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "proc_w = int(orig_w * RESIZE_FACTOR)\n",
    "proc_h = int(orig_h * RESIZE_FACTOR)\n",
    "if proc_w <= 0 or proc_h <= 0:\n",
    "    raise ValueError('Invalid RESIZE_FACTOR generated non-positive dimensions.')\n",
    "\n",
    "writer = cv2.VideoWriter(video_out, cv2.VideoWriter_fourcc(*'mp4v'), fps, (proc_w, proc_h))\n",
    "tracker = CentroidTracker(max_disappeared=20, max_distance=100.0)\n",
    "frame_records = []\n",
    "\n",
    "if num_frames <= 0:\n",
    "    print('[WARN] CAP_PROP_FRAME_COUNT returned 0/unknown. Falling back to streaming loop.')\n",
    "\n",
    "print(f'[INFO] Processing video @ {fps:.2f} FPS | size {orig_w}x{orig_h} -> {proc_w}x{proc_h}')\n",
    "\n",
    "frame_idx = 0\n",
    "pbar = tqdm(total=num_frames if num_frames > 0 else None, desc='Video Processing')\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    if RESIZE_FACTOR != 1.0:\n",
    "        frame = cv2.resize(frame, (proc_w, proc_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    active_ids = sorted(list(tracker.objects.keys()))\n",
    "\n",
    "    # Skip-frame logic for speed, but still export a per-frame record.\n",
    "    if frame_idx % PROCESS_EVERY_N_FRAMES != 0:\n",
    "        frame_records.append({'frame_idx': frame_idx, 'unique_ids': active_ids, 'count': len(active_ids)})\n",
    "        cv2.putText(frame, f'Count: {len(active_ids)}', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)\n",
    "        writer.write(frame)\n",
    "        frame_idx += 1\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        pallet_boxes, _, _ = detect_objects(frame, prompt='pallet')\n",
    "        box_boxes, _, _ = detect_objects(frame, prompt='box')\n",
    "\n",
    "        masks = filter_masks(segment_boxes(frame, box_boxes))\n",
    "        centroids = [c for c in (get_centroid(m) for m in masks) if c is not None]\n",
    "        tracked = tracker.update(centroids)\n",
    "        active_ids = sorted(list(tracked.keys()))\n",
    "\n",
    "        if DRAW_MASK_OVERLAY and len(masks) > 0:\n",
    "            overlay = frame.copy()\n",
    "            for m in masks:\n",
    "                overlay[m.astype(bool)] = (0.5 * overlay[m.astype(bool)] + 0.5 * np.array([0, 255, 0], dtype=np.uint8)).astype(np.uint8)\n",
    "            frame = cv2.addWeighted(overlay, 0.35, frame, 0.65, 0)\n",
    "\n",
    "        for bx in pallet_boxes:\n",
    "            x1, y1, x2, y2 = map(int, bx)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 200, 0), 2)\n",
    "            cv2.putText(frame, 'Pallet', (x1, max(0, y1 - 8)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 200, 0), 2)\n",
    "\n",
    "        for bx in box_boxes:\n",
    "            x1, y1, x2, y2 = map(int, bx)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 220, 0), 1)\n",
    "\n",
    "        for object_id, state in tracked.items():\n",
    "            cx, cy = state.centroid\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f'ID {object_id}', (cx + 5, cy - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Keep pipeline alive and keep exporting a row for this frame.\n",
    "        print(f'[WARN] Frame {frame_idx}: {e}')\n",
    "        active_ids = sorted(list(tracker.objects.keys()))\n",
    "\n",
    "    frame_records.append({'frame_idx': frame_idx, 'unique_ids': active_ids, 'count': len(active_ids)})\n",
    "    cv2.putText(frame, f'Count: {len(active_ids)}', (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)\n",
    "\n",
    "    writer.write(frame)\n",
    "\n",
    "    if frame_idx % LOG_EVERY_N_FRAMES == 0:\n",
    "        if num_frames > 0:\n",
    "            print(f'[INFO] frame={frame_idx}/{num_frames} | tracked={len(tracker.objects)}')\n",
    "        else:\n",
    "            print(f'[INFO] frame={frame_idx} | tracked={len(tracker.objects)}')\n",
    "\n",
    "    cleanup_cuda()\n",
    "    frame_idx += 1\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "cap.release()\n",
    "writer.release()\n",
    "cleanup_cuda()\n",
    "\n",
    "print(f'[INFO] Annotated video saved to: {video_out}')\n",
    "print(f'[INFO] Records exported from pipeline: {len(frame_records)}')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 6 — JSON Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 6 — JSON Export\n",
    "# =========================\n",
    "with open(json_out, 'w', encoding='utf-8') as f:\n",
    "    json.dump(frame_records, f, indent=2)\n",
    "\n",
    "print(f'[INFO] JSON saved to: {json_out}')\n",
    "print(f'[INFO] Total exported frames: {len(frame_records)}')\n",
    "if len(frame_records) == 0:\n",
    "    print('[WARN] No frames were exported. Verify video path/codec and inspect frame read logs.')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOCK 7 — Interactive Analytics Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 7 — Interactive Analytics Playground\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "with open(json_out, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "if df.empty:\n",
    "    raise ValueError('No records found in counts.json')\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df['frame_idx'], df['count'], color='tab:blue', linewidth=1.5)\n",
    "plt.title('Tracked Object Count per Frame')\n",
    "plt.xlabel('Frame Index')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    for obj_id in row['unique_ids']:\n",
    "        rows.append({'frame_idx': int(row['frame_idx']), 'object_id': int(obj_id), 'presence': 1})\n",
    "\n",
    "id_df = pd.DataFrame(rows)\n",
    "if not id_df.empty:\n",
    "    fig = px.scatter(id_df, x='frame_idx', y='object_id', color='object_id',\n",
    "                     title='Object ID Presence Over Time', opacity=0.8, height=520)\n",
    "    fig.update_traces(marker=dict(size=6))\n",
    "    fig.update_layout(xaxis_title='Frame Index', yaxis_title='Tracked ID', showlegend=False)\n",
    "    fig.show()\n",
    "else:\n",
    "    print('[INFO] No active IDs to visualize.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}